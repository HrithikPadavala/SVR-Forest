# SVR-Forest for Imbalanced Learning
In this paper, we have introduced an approach that uses the bagging of Surface to Volume regularized(SVR) Trees. In our proposed model, we have constructed a classifier from the given training data(the input). For any given training set T, the proposed model forms bootstrap training sets Tn where n is a parameter to our model. From these n bootstrapped samples, the model constructs n optimal SVR Tree classifiers and finally fetch the votes to form a bagged predictor. The optimal SVR Tree in the sense, the SVoRF model grows each tree with the best possible penalty parameter that is suitable for the current training input sample. In case of classification problem, SVoRF outputs the majority vote that has been predicted and in the case of aggregation, it outputs the average vote. The main reason behind using a Random Forest of SVR Trees instead of single SVR Tree is to enhance the accuracy and to improvise the generalisation error as well. This was proved in our experimental analysis with appropriate competitive methods and numerous datasets. Therefore, Random forest algorithm uses randomization to rebuild several training subsets from the training data and builds an SVR tree for each random subset and finally aggregates the decisions over all the trees of the forest.
